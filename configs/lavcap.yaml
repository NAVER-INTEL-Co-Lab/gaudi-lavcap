model:
  # paths
  encoder_path: "pretrained_weights/audiotransformer_base_mAP_4999.pt"
  freeze_enc: True
  llama_decoder_path: "/mnt/lynx4/datasets/ltu_as/tedlium/Video-LLaMA-2-7B-Finetuned/llama-2-7b-chat-hf"  
  # ckpt: "pretrained_weights/checkpoint_26.pth" # if not "", load model from ckpt for training or evaluation

  # encoder (CED)
  n_class: 527
  embed_dim: 768
  vis_embed_dim: 768
  melbins: 64
  target_length: 1024

  # Encoder LoRA
  enc_lora: True
  enc_lora_rank: 8
  enc_lora_alpha: 32
  enc_lora_dropout: 0.1

  # Decoder LoRA
  dec_lora: True
  dec_lora_rank: 8
  dec_lora_alpha: 32
  dec_lora_dropout: 0.1

  # Fusion (Optimal Transport)
  fusion_type: "ot_tempcat_QAV_add" # "temp_cat_afQF", "temp_cat_bfQF", "chan_cat_bfQF", "", "aud_embed_only", "vis_embed_only", "opt_trans_temp", "opt_trans_chan"
  spec_f_mean: True
  av_embed_norm: False
  ot_loss_weight: 1.0
  num_sinkhorn_iter: 50
  sinkhorn_epsilon: 0.1
  prompt_ratio: 0
  use_exp_Q: False

  # Q-former
  use_speech_Qformer: False
  num_speech_query_token: 1
  freeze_speech_QFormer: False
  window_level_Qformer: True
  second_per_window: 0.333333
  second_stride: 0.333333
  
  # Proj Q-former to LLaMA
  speech_llama_proj_model: ""
  freeze_speech_llama_proj: False

  # Prompting
  multi_prompt: True
  prompt_template: "USER: {}\nASSISTANT:"
  prompt_path: "./prompts/train_prompt.json"
  test_prompt_path: "./prompts/test_prompt.json"
  max_txt_len: 300
  end_sym: "</s>"

  # captioning model
  loss_type: 
  padding_idx: 0

  # details for LLaMA
  low_resource: False
  device_8bit: 0

datasets:
  train_data_path: "/mnt/lynx2/datasets/audiocaps/train"
  valid_data_path: "/mnt/lynx2/datasets/audiocaps/test"
  test_data_path: 
  label_path: "dataset/class_labels_indices.csv"
  annot_path: "/mnt/lynx2/datasets/audiocaps/test_coco.json"

  target_length: 1024
  melbins: 64   # edited by khrho (default: 128)
  freqm: 48
  timem: 192

  dataset_mean: -4.346
  dataset_std: 4.332
  label_smooth: 0.1

  total_frame: 20
  num_frame: 1
  im_res: 224

run:
  do_eval: False # if True, only evaluate model on test data
  do_debug: False
  # log & settings
  seed: 42
  # output_dir: "pretrained_weights"
  output_dir: "/mnt/bear2/users/jongbin/results/mmcap_final_wd_2"
  accum_grad_iters: 1

  train_batch_size: 4
  eval_batch_size: 1
  num_workers: 8

  # optimizer & scheduler
  optims:
    max_epoch: 100
    warmup_ratio: 0.02
    warmup_start_lr: 2e-9
    init_lr: 5e-6
    min_lr: 1e-8
    weight_decay: 1e-6  
    beta2: 0.999

  wandb:
    no_wandb: False
    wandb_entity: "jongbin"
    wandb_project: "audiocaps"
    wandb_name: "gaudi_final"
